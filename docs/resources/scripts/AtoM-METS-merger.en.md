# Metadata Integration Package for Archivematica and AtoM

[:material-account: Seibt, Yonathan Jérôme](https://infoscience.epfl.ch/entities/person/54236519-4a6b-461d-a338-60e3627f13d0)  
:material-calendar: December 2024  
:material-office-building: Archives de la construction moderne – EPFL

*This package is distributed under the GNU General Public License v3. You may redistribute and/or modify it under the terms of the GNU GPL as published by the Free Software Foundation, either version 3 of the license, or (at your option) any later version.*  
*This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.*  
*To obtain a copy of the GNU General Public License, visit [www.gnu.org/licenses/](https://www.gnu.org/licenses/).*

---

## Purpose

This package automates the integration of metadata generated by Archivematica (1.17.0) into AtoM (2.8.2), ensuring smooth synchronization and consistent updates between both systems. It provides a structured workflow for:

1. Data extraction and transformation: Merging metadata from METS (XML) files generated by Archivematica with records exported from AtoM (CSV).
2. Automated record updates in AtoM: Injecting enriched metadata via HTTP interactions.

---

## Contents

**Python scripts**

- `AtoM_METS_Data_Merger.py`: Parses and merges metadata from METS (Archivematica) and CSV (AtoM).
- `AtoM_Record_Updater.py`: Updates AtoM records with enriched metadata via the web interface.

**Data files**

- `isad_0000000001.csv`: Exported AtoM file containing existing records.
- `METS_123456789.xml`: METS file produced by Archivematica containing structured metadata.
- `login.csv`: Credentials for accessing AtoM’s web interface.
- `urls.csv`: List of URLs or UUIDs needed to locate and synchronize data.

---

## Technical requirements

- **Systems required**

  - Archivematica: Version 1.17.0, for generating METS files.
  - AtoM: Version 2.8.2

- **Python environment, version 3.8+ with the following libraries:**

  - `pandas`: For CSV file handling.
  - `xml.etree.ElementTree`: For parsing XML files.
  - `requests`: For sending HTTP requests.
  - `BeautifulSoup (bs4)`: For navigating web forms.

---

## Why is this script necessary?

When a DIP generated by Archivematica is pushed to AtoM, the folder structure is flattened: all files appear at the same level, losing their original hierarchy. This limitation is documented in the Archivematica/AtoM communities.

This script helps re-establish a logical link between:

- Existing AtoM records (exported via Clipboard in CSV format)
- Files in the DIP (and their metadata, extracted from the METS file)

The resulting file, `urls.csv`, can then be used by a second script to automatically inject enriched metadata into AtoM.

---

## How to use

### Check the name of the METS file

Open the script and make sure the `mets_filename` variable matches the name of your METS file. For example:

```py title="AtoM_METS_Data_Merger.py"
mets_filename = "METS_123456789.xml"
```

Edit this value if needed.

### Open a command prompt and navigate to the correct folder

```bash
cd "C:\Users\your.name\Path\To\The\Folder"
```

This folder must contain:

- The METS file
- The AtoM CSV export
- The Python script

### Run the script

```bash
python AtoM_METS_Data_Merger.py
```

If everything goes well, the following message should appear:

```bash
Le fichier 'urls.csv' a été créé avec succès.
```

### Expected output

The generated `urls.csv` contains the following columns:

| Column                | Description                                                 |
| --------------------- | ----------------------------------------------------------- |
| `urls`                | Two URLs per AtoM record (FR and EN) used to apply updates. |
| `titre`               | Title constructed from the original filename.               |
| `editEvents_0_type`   | Type of event to register, e.g., `"creation"`.              |
| `dateCreation`        | File creation date extracted from the METS file.            |
| `levelOfDescription`  | ISAD-G level of description, e.g., `"item"`.                |
| `locationOfOriginals` | Original file path inside the DIP.                          |
| `extentAndMedium`     | File type and size (e.g., PDF, 512 KB).                     |
| `identifier`          | ISAD-G reference code generated for the item.               |

---

## Next steps

You can now use the generated `urls.csv` with the `AtoM_Record_Updater.py` script to inject metadata directly into AtoM’s web interface.

### Script `AtoM_METS_Data_Merger.py`

*(The script contents follow here and remain unchanged from the original French version.)*

```py
# Script: AtoM_METS_Data_Merger.py
# Description: This script extracts and merges metadata from AtoM (CSV) and METS (XML) files. It standardizes, parses, and combines data
# into a unified, enriched CSV file ready for further processing or import into AtoM.
#
# Author: Yonathan Seibt, Archives de la construction moderne – EPFL
# Date: November 2024
#
# License: This script is distributed under the GNU General Public License v3. You may redistribute and/or modify it under the terms 
#          of the GNU GPL as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version.
#
#          This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty 
#          of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details.
#
#          You should have received a copy of the GNU General Public License along with this program. If not, see <https://www.gnu.org/licenses/>.

import pandas as pd
import re
import xml.etree.ElementTree as ET
from datetime import datetime

# ---- Partie 1 : Extraire les UUID du fichier CSV ----

# Lire le fichier CSV
df_csv = pd.read_csv('isad_0000000001.csv')  # Remplacez par le nom de votre fichier CSV

# Ajouter une colonne d'index pour conserver l'ordre
df_csv['index'] = df_csv.index

# Fonction pour extraire l'UUID
def extract_uuid(url):
    if isinstance(url, str):
        match = re.search(r'/([0-9a-fA-F-]{36})-', url)
        if match:
            return match.group(1)
    return None

# Appliquer la fonction à la colonne contenant les URLs
df_csv['UUID'] = df_csv['digitalObjectURI'].apply(extract_uuid)  # Remplacez par le nom de votre colonne

# ---- Partie 2 : Extraire les données du fichier XML ----

# Charger le fichier XML
xml_file = 'METS.07fdd110-6ae2-49c4-989d-6394c152be9c.xml'
tree = ET.parse(xml_file)
root = tree.getroot()

# Définir l'espace de noms utilisé dans le fichier XML
ns = {'mets': 'http://www.loc.gov/METS/'}

# Fonction pour extraire toutes les données des balises <mets:amdSec> et les structurer
def extract_all_amdSec_data(root, ns):
    amdSec_data = []
    
    # Trouver toutes les balises <mets:amdSec>
    for amdSec in root.findall('.//mets:amdSec', ns):
        data = {}
        
        # Extraire l'attribut 'ID' de la balise <mets:amdSec>
        data['ID'] = amdSec.attrib.get('ID', None)

        # Extraire toutes les sous-balises pertinentes
        for element in amdSec.iter():
            # Si l'élément est une balise avec un texte (et non l'attribut)
            if element.tag != '{http://www.loc.gov/METS/}amdSec' and element.text:
                # Enlever l'espace de noms de la balise
                tag = element.tag.split('}')[1] if '}' in element.tag else element.tag
                data[tag] = element.text.strip()
        
        # Ajouter les données extraites à la liste
        amdSec_data.append(data)
    
    return amdSec_data

# Extraire toutes les données des balises <mets:amdSec>
amdSec_data = extract_all_amdSec_data(root, ns)

# Créer un DataFrame avec les données extraites
df_xml = pd.DataFrame(amdSec_data)

# Filtrer les colonnes non pertinentes
columns_to_keep = [
    'ID', 'objectIdentifierValue', 'size', 'messageDigestAlgorithm', 'messageDigest', 'formatName', 'formatVersion', 
    'formatRegistryKey', 'dateCreatedByApplication', 'created', 
    'creatingApplicationName', 'FileName', 'FileType', 'FileTypeExtension', 'MIMEType', 'originalName'
]

# Garder uniquement les colonnes pertinentes
df_filtered = df_xml[columns_to_keep]

# ---- Partie 3 : Fusionner les données des deux DataFrames ----

# Fusionner les deux DataFrames sur les colonnes correspondantes
# Ici, on suppose que les noms des colonnes à fusionner sont respectivement 'objectIdentifierValue' et 'UUID'
df_merged = pd.merge(df_filtered, df_csv, left_on='objectIdentifierValue', right_on='UUID', how='inner')

# Réordonner le DataFrame fusionné selon l'index original
df_merged = df_merged.sort_values(by='index')

# ---- Partie 4 : Transformation et duplication des données ----

# Initialiser les nouvelles colonnes
df_merged['urls'] = 'https://morphe-test.epfl.ch/index.php/' + df_merged['slug'] + '/edit'
df_merged['titre'] = df_merged['title'] + ' (fichier numérique)'
df_merged['editEvents_0_type'] = 'creation'

# Corriger le format de date
def format_date(date_str):
    try:
        return datetime.strptime(date_str, '%Y-%m-%dT%H:%M:%SZ').strftime('%Y-%m-%d')
    except ValueError:
        return ''

df_merged['dateCreation'] = df_merged['dateCreatedByApplication'].apply(format_date)
df_merged['levelOfDescription'] = 'item'
df_merged['scopeAndContent'] = 'Chemin d’accès du fichier sur le support original : ' + df_merged['originalName'].str.replace('%transferDirectory%objects/', '')
df_merged['extentAndMedium'] = '1 fichier numérique ' + df_merged['formatName'] + ' de ' + df_merged['size'] + ' octets'
df_merged['identifier'] = ['0219.01.0130/04.01.01.' + str(i+1).zfill(2) for i in range(len(df_merged))]

# Dupliquer chaque enregistrement avec modification de la colonne urls
df_duplicated = df_merged.copy()
df_duplicated['urls'] = 'https://morphe-test.epfl.ch/index.php/' + df_duplicated['slug'] + '/edit?sf_culture=fr&template=isad'

# Combiner les DataFrames
df_final = pd.concat([df_merged, df_duplicated], ignore_index=True)

# Réordonner le DataFrame final selon l'index original
df_final = df_final.sort_values(by='index')

# Sélectionner les colonnes nécessaires
columns_to_keep = ['urls', 'titre', 'editEvents_0_type', 'dateCreation', 'levelOfDescription', 'scopeAndContent', 'extentAndMedium', 'identifier']
df_final = df_final[columns_to_keep]

# Sauvegarder dans le fichier CSV final
output_file = 'urls.csv'
df_final.to_csv(output_file, index=False)

print(f"Le fichier '{output_file}' a été créé avec succès.")
```